# agent/nodes.py
import requests
import serverconfig

class InputParserNode:
    def process(self, context):
        print("[NODE] InputParserNode: Parsing request...")
        # ... (logic to parse request_data and populate context) ...
        return context

class MemoryRecallNode:
    def __init__(self, ltm):
        self.ltm = ltm
    def process(self, context):
        print("[NODE] MemoryRecallNode: Querying long-term memory...")
        # ... (logic to recall memories and add to context) ...
        return context

class PromptFormatterNode:
    def __init__(self, wm):
        self.wm = wm
    def process(self, context):
        print("[NODE] PromptFormatterNode: Building final prompt...")
        # ... (logic to build the final prompt/messages list and add to context) ...
        return context

class LLMCallNode:
    def process(self, context):
        """Sends the final payload to the Llama.cpp server."""
        print("[NODE] LLMCallNode: Sending request to core model...")
        payload = context.get('llm_payload')
        if not payload:
            context['llm_response'] = "Error: No payload generated by previous nodes."
            return context

        try:
            # Use the correct endpoint based on payload structure
            if "messages" in payload:
                url = serverconfig.LLAMA_CPP_CHAT_URL
            else:
                url = serverconfig.LLAMA_CPP_COMPLETION_URL

            response = requests.post(url, json=payload, headers={'Content-Type': 'application/json'}, timeout=180)
            response.raise_for_status()

            if "messages" in payload:
                ai_response_text = response.json()['choices'][0]['message']['content'].strip()
            else:
                ai_response_text = response.json().get('content', 'Error: No content in response.').strip()

            context['llm_response'] = ai_response_text
            print(f"[LLM_RESPONSE] Received: '{ai_response_text[:60]}...'")
        except requests.exceptions.RequestException as e:
            error_msg = f"FATAL: Error communicating with Llama.cpp server: {e}"
            print(error_msg)
            context['llm_response'] = "Error: Could not connect to the language model."
        
        return context